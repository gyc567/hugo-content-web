#!/usr/bin/env python3
"""
Product Huntä»Šæ—¥TOP3äº§å“è‡ªåŠ¨åˆ†æå’Œæ–‡ç« ç”Ÿæˆå™¨
æ¯æ—¥æŠ“å–Product Huntçš„Top Products Launching Todayæ¦œå•å‰ä¸‰åï¼Œç”Ÿæˆä¸“ä¸šè¯„æµ‹æ–‡ç« 
"""

import requests
import json
import os
import datetime
from typing import List, Dict, Any, Set
import time
import re
import hashlib
from bs4 import BeautifulSoup


class ProductHuntAnalyzer:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive'
        }
        
        # äº§å“å†å²è®°å½•æ–‡ä»¶è·¯å¾„
        self.history_file = 'data/producthunt_products.json'
        self.ensure_data_directory()
    
    def ensure_data_directory(self):
        """ç¡®ä¿dataç›®å½•å­˜åœ¨"""
        os.makedirs('data', exist_ok=True)
        os.makedirs('content/posts', exist_ok=True)
    
    def load_analyzed_products(self) -> Set[str]:
        """åŠ è½½å·²åˆ†æçš„äº§å“å†å²è®°å½•"""
        try:
            if os.path.exists(self.history_file):
                with open(self.history_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return set(data.get('analyzed_products', []))
            return set()
        except Exception as e:
            print(f"âš ï¸  åŠ è½½äº§å“å†å²è®°å½•å¤±è´¥: {e}")
            return set()
    
    def save_analyzed_products(self, analyzed_products: Set[str]):
        """ä¿å­˜å·²åˆ†æçš„äº§å“å†å²è®°å½•"""
        try:
            data = {
                'last_updated': datetime.datetime.now().isoformat(),
                'analyzed_products': list(analyzed_products),
                'total_products': len(analyzed_products)
            }
            with open(self.history_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜äº§å“å†å²è®°å½•å¤±è´¥: {e}")
    
    def fetch_top_products(self) -> List[Dict]:
        """æŠ“å–Product Huntä»Šæ—¥TOP3äº§å“"""
        url = "https://www.producthunt.com"
        
        try:
            print("ğŸ” æ­£åœ¨æŠ“å–Product Huntä»Šæ—¥çƒ­é—¨äº§å“...")
            response = requests.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            products = []
            
            # æŸ¥æ‰¾äº§å“åˆ—è¡¨å…ƒç´ 
            product_elements = soup.find_all('div', class_=re.compile(r'styles_item__.*'))
            
            if not product_elements:
                # å¤‡ç”¨é€‰æ‹©å™¨
                product_elements = soup.find_all(['div', 'article'], attrs={'data-test': re.compile(r'post.*')})
            
            count = 0
            for element in product_elements:
                if count >= 3:  # åªå–å‰3ä¸ª
                    break
                
                product = self._extract_product_info(element)
                if product and product.get('name'):
                    products.append(product)
                    count += 1
            
            print(f"âœ… æˆåŠŸæŠ“å–åˆ° {len(products)} ä¸ªäº§å“")
            return products
            
        except Exception as e:
            print(f"âŒ æŠ“å–Product Huntå¤±è´¥: {e}")
            return []
    
    def _extract_product_info(self, element) -> Dict:
        """ä»HTMLå…ƒç´ ä¸­æå–äº§å“ä¿¡æ¯"""
        try:
            product = {}
            
            # æå–äº§å“åç§°
            name_element = element.find(['h3', 'h2', 'a'], string=re.compile(r'.+'))
            if not name_element:
                name_element = element.find('a', href=re.compile(r'/posts/'))
            
            if name_element:
                product['name'] = name_element.get_text(strip=True)
                
                # æå–äº§å“é“¾æ¥
                if name_element.name == 'a':
                    href = name_element.get('href', '')
                    product['url'] = f"https://www.producthunt.com{href}" if href.startswith('/') else href
                else:
                    link_element = element.find('a', href=re.compile(r'/posts/'))
                    if link_element:
                        href = link_element.get('href', '')
                        product['url'] = f"https://www.producthunt.com{href}" if href.startswith('/') else href
            
            # æå–æè¿°
            desc_element = element.find('span', string=re.compile(r'.{10,}'))
            if not desc_element:
                desc_element = element.find(['p', 'div'], string=re.compile(r'.{10,}'))
            
            if desc_element:
                product['description'] = desc_element.get_text(strip=True)
            
            # æå–ç‚¹èµæ•°
            votes_element = element.find(string=re.compile(r'\d+'))
            if votes_element:
                votes_match = re.search(r'(\d+)', votes_element)
                if votes_match:
                    product['votes'] = int(votes_match.group(1))
            
            # è®¾ç½®é»˜è®¤å€¼
            product.setdefault('description', 'æš‚æ— æè¿°')
            product.setdefault('votes', 0)
            product.setdefault('url', 'https://www.producthunt.com')
            
            return product
            
        except Exception as e:
            print(f"âš ï¸  æå–äº§å“ä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    def get_product_details(self, product: Dict) -> Dict:
        """è·å–äº§å“è¯¦ç»†ä¿¡æ¯"""
        try:
            if not product.get('url') or product['url'] == 'https://www.producthunt.com':
                return product
            
            print(f"ğŸ“ è·å–äº§å“è¯¦æƒ…: {product['name']}")
            response = requests.get(product['url'], headers=self.headers, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æå–æ›´è¯¦ç»†çš„æè¿°
            desc_elements = soup.find_all(['p', 'div'], string=re.compile(r'.{20,}'))
            detailed_description = ""
            for desc in desc_elements[:2]:  # å–å‰ä¸¤ä¸ªè¾ƒé•¿çš„æè¿°
                text = desc.get_text(strip=True)
                if len(text) > len(detailed_description):
                    detailed_description = text
            
            if detailed_description and len(detailed_description) > len(product.get('description', '')):
                product['detailed_description'] = detailed_description[:500]
            
            # æå–æ ‡ç­¾
            tag_elements = soup.find_all('span', class_=re.compile(r'tag'))
            if not tag_elements:
                tag_elements = soup.find_all('a', href=re.compile(r'/topics/'))
            
            tags = []
            for tag in tag_elements[:5]:  # æœ€å¤š5ä¸ªæ ‡ç­¾
                tag_text = tag.get_text(strip=True)
                if tag_text and len(tag_text) < 20:
                    tags.append(tag_text)
            
            product['tags'] = tags
            
            return product
            
        except Exception as e:
            print(f"âš ï¸  è·å–äº§å“è¯¦æƒ…å¤±è´¥ {product.get('name', 'Unknown')}: {e}")
            return product
    
    def analyze_product_quality(self, product: Dict) -> Dict:
        """åˆ†æäº§å“è´¨é‡"""
        score = 0
        max_score = 100
        analysis = {
            'overall_score': 0,
            'strengths': [],
            'weaknesses': [],
            'recommendations': []
        }
        
        # è¯„åˆ†æ ‡å‡†
        votes = product.get('votes', 0)
        if votes > 500:
            score += 30
            analysis['strengths'].append(f"é«˜äººæ°”äº§å“ ({votes} votes)")
        elif votes > 200:
            score += 25
            analysis['strengths'].append(f"ä¸­ç­‰äººæ°” ({votes} votes)")
        elif votes > 50:
            score += 15
        else:
            analysis['weaknesses'].append("æŠ•ç¥¨æ•°è¾ƒå°‘ï¼Œå¯èƒ½æ˜¯æ–°äº§å“")
        
        # æè¿°è´¨é‡
        description = product.get('detailed_description') or product.get('description', '')
        if len(description) > 100:
            score += 20
            analysis['strengths'].append("äº§å“æè¿°è¯¦ç»†")
        elif len(description) > 50:
            score += 10
        else:
            analysis['weaknesses'].append("äº§å“æè¿°ä¸å¤Ÿè¯¦ç»†")
            analysis['recommendations'].append("å»ºè®®å®Œå–„äº§å“ä»‹ç»")
        
        # æ ‡ç­¾å®Œæ•´æ€§
        tags = product.get('tags', [])
        if len(tags) >= 3:
            score += 15
            analysis['strengths'].append("äº§å“æ ‡ç­¾ä¸°å¯Œ")
        elif len(tags) >= 1:
            score += 10
        else:
            analysis['weaknesses'].append("ç¼ºå°‘äº§å“æ ‡ç­¾")
        
        # äº§å“åç§°è´¨é‡
        name = product.get('name', '')
        if len(name) > 5 and len(name) < 30:
            score += 10
            analysis['strengths'].append("äº§å“åç§°ç®€æ´æ˜äº†")
        
        # URLæœ‰æ•ˆæ€§
        if product.get('url') and product['url'] != 'https://www.producthunt.com':
            score += 15
            analysis['strengths'].append("äº§å“é“¾æ¥å®Œæ•´")
        else:
            analysis['weaknesses'].append("ç¼ºå°‘äº§å“è¯¦æƒ…é“¾æ¥")
        
        analysis['overall_score'] = min(score, max_score)
        
        # ç”Ÿæˆæ¨è
        if analysis['overall_score'] > 80:
            analysis['recommendations'].append("ä¼˜ç§€çš„Product Huntäº§å“ï¼Œå€¼å¾—å…³æ³¨")
        elif analysis['overall_score'] > 60:
            analysis['recommendations'].append("ä¸é”™çš„äº§å“ï¼Œå…·æœ‰ä¸€å®šå¸‚åœºæ½œåŠ›")
        else:
            analysis['recommendations'].append("äº§å“æœ‰å¾…å®Œå–„ï¼Œå¯å…³æ³¨åç»­å‘å±•")
        
        return analysis
    
    def generate_article(self, products: List[Dict]) -> bool:
        """ç”Ÿæˆè¯„æµ‹æ–‡ç« """
        if not products:
            print("ğŸ“ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„äº§å“ï¼Œè·³è¿‡æ–‡ç« ç”Ÿæˆ")
            return False
        
        date_str = datetime.datetime.now().strftime('%Y-%m-%d')
        title = f"Product Huntä»Šæ—¥TOP3çƒ­é—¨äº§å“æ¨è - {date_str}"
        filename = f"producthunt-top3-review-{date_str}.md"
        filepath = f"content/posts/{filename}"
        
        # æ–‡ç« å†…å®¹
        content = f"""---
title: "{title}"
date: {datetime.datetime.now().isoformat()}
draft: false
description: "æ¯æ—¥ç²¾é€‰Product Huntçƒ­é—¨äº§å“TOP3ï¼Œæ·±åº¦åˆ†æäº§å“ç‰¹è‰²ã€å¸‚åœºå®šä½å’Œç”¨æˆ·ä»·å€¼"
keywords: ["Product Hunt", "çƒ­é—¨äº§å“", "äº§å“æ¨è", "åˆ›ä¸šé¡¹ç›®", "ç§‘æŠ€äº§å“"]
categories: ["Product Huntçƒ­é—¨"]
tags: ["Product Hunt", "äº§å“è¯„æµ‹", "åˆ›ä¸šé¡¹ç›®", "ç§‘æŠ€åˆ›æ–°", "çƒ­é—¨åº”ç”¨"]
image: "/images/producthunt-top3.jpg"
---

## ğŸ† Product Huntä»Šæ—¥TOP3äº§å“æ¦‚è§ˆ

ä»Šå¤©ä¸ºå¤§å®¶ç²¾é€‰Product Huntä¸Šæœ€å—å…³æ³¨çš„ {len(products)} æ¬¾äº§å“ã€‚è¿™äº›äº§å“ä»£è¡¨äº†å½“å‰ç§‘æŠ€åˆ›æ–°çš„å‰æ²¿è¶‹åŠ¿ï¼Œæ¶µç›–äº†ä»å·¥å…·åº”ç”¨åˆ°åˆ›æ–°æœåŠ¡çš„å„ä¸ªé¢†åŸŸã€‚

"""
        
        for i, product in enumerate(products, 1):
            analysis = product.get('analysis', {})
            
            content += f"""
## {i}. {product['name']}

**ğŸ‘ æŠ•ç¥¨æ•°:** {product.get('votes', 0)} | **â­ è´¨é‡è¯„åˆ†:** {analysis.get('overall_score', 0)}/100

**ğŸ”— äº§å“é“¾æ¥:** [{product['name']}]({product.get('url', 'https://www.producthunt.com')})

### äº§å“ç®€ä»‹

{product.get('detailed_description') or product.get('description', 'æš‚æ— è¯¦ç»†æè¿°')}

### äº§å“æ ‡ç­¾

{', '.join(product.get('tags', [])) if product.get('tags') else 'æš‚æ— æ ‡ç­¾'}

### è´¨é‡è¯„ä¼°

#### äº§å“ä¼˜åŠ¿
{chr(10).join(f"- {strength}" for strength in analysis.get('strengths', []))}

#### éœ€è¦æ”¹è¿›
{chr(10).join(f"- {weakness}" for weakness in analysis.get('weaknesses', []))}

#### æ¨èå»ºè®®
{chr(10).join(f"- {rec}" for rec in analysis.get('recommendations', []))}

---

"""
        
        content += f"""

## ğŸ“Š ä»Šæ—¥è¶‹åŠ¿åˆ†æ

æœ¬æœŸå…±è¯„æµ‹äº† {len(products)} æ¬¾Product Huntçƒ­é—¨äº§å“ï¼š

- **å¹³å‡æŠ•ç¥¨æ•°:** {sum(p.get('votes', 0) for p in products) / len(products):.0f}
- **å¹³å‡è´¨é‡è¯„åˆ†:** {sum(p.get('analysis', {}).get('overall_score', 0) for p in products) / len(products):.0f}/100
- **çƒ­é—¨æ ‡ç­¾:** {', '.join(set([tag for p in products for tag in p.get('tags', [])[:3]]))}

## ğŸ’¡ äº§å“æ´å¯Ÿ

### å¸‚åœºè¶‹åŠ¿
å½“å‰Product Huntä¸Šçš„çƒ­é—¨äº§å“å‘ˆç°ä»¥ä¸‹ç‰¹ç‚¹ï¼š
1. **AIé©±åŠ¨:** äººå·¥æ™ºèƒ½ç›¸å…³äº§å“æŒç»­ç«çƒ­
2. **æ•ˆç‡å·¥å…·:** æå‡å·¥ä½œæ•ˆç‡çš„å·¥å…·å—åˆ°é’ç  
3. **ç”¨æˆ·ä½“éªŒ:** æ³¨é‡ç”¨æˆ·ä½“éªŒè®¾è®¡çš„äº§å“æ›´å®¹æ˜“æˆåŠŸ

### é€‰å“å»ºè®®
- **åˆ›ä¸šè€…:** å…³æ³¨ç”¨æˆ·çœŸå®éœ€æ±‚ï¼Œé¿å…è¿‡åº¦å¤æ‚åŒ–
- **æŠ•èµ„äºº:** é‡ç‚¹å…³æ³¨æœ‰æ¸…æ™°å•†ä¸šæ¨¡å¼çš„äº§å“
- **ç”¨æˆ·:** é€‰æ‹©è§£å†³å®é™…é—®é¢˜çš„å·¥å…·ï¼Œè€Œéè¿½æ±‚æ–°å¥‡

## ğŸ”” å…³æ³¨æ›´æ–°

æˆ‘ä»¬æ¯å¤©éƒ½ä¼šå…³æ³¨Product Huntä¸Šçš„æœ€æ–°çƒ­é—¨äº§å“ï¼Œä¸ºå¤§å®¶æä¾›æœ€åŠæ—¶çš„äº§å“åŠ¨æ€å’Œæ·±åº¦åˆ†æã€‚è®°å¾—å…³æ³¨æˆ‘ä»¬çš„æ›´æ–°ï¼

---

*æœ¬æ–‡ç”±è‡ªåŠ¨åŒ–åˆ†æç³»ç»Ÿç”Ÿæˆï¼Œæ•°æ®æ¥æºäºProduct Huntï¼Œæ›´æ–°æ—¶é—´ï¼š{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        # å†™å…¥æ–‡ä»¶
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"âœ… æˆåŠŸç”Ÿæˆæ–‡ç« : {filename}")
            return True
        except Exception as e:
            print(f"âŒ ç”Ÿæˆæ–‡ç« å¤±è´¥: {e}")
            return False
    
    def run_analysis(self, max_products: int = 3) -> bool:
        """è¿è¡Œå®Œæ•´çš„åˆ†ææµç¨‹"""
        print("ğŸš€ å¼€å§‹Product Hunt TOP3äº§å“åˆ†æ...")
        
        # åŠ è½½å†å²è®°å½•
        analyzed_products = self.load_analyzed_products()
        print(f"ğŸ“š å·²åˆ†æäº§å“æ•°é‡: {len(analyzed_products)}")
        
        # è·å–ä»Šæ—¥TOPäº§å“
        products = self.fetch_top_products()
        
        if not products:
            print("ğŸ“ ä»Šæ—¥æ— æ³•è·å–Product Huntäº§å“æ•°æ®")
            return False
        
        new_products = []
        today_str = datetime.datetime.now().strftime('%Y-%m-%d')
        
        for product in products:
            product_id = f"{product['name']}-{today_str}"
            if product_id not in analyzed_products:
                # è·å–è¯¦ç»†ä¿¡æ¯
                detailed_product = self.get_product_details(product)
                # è¿›è¡Œè´¨é‡åˆ†æ
                detailed_product['analysis'] = self.analyze_product_quality(detailed_product)
                new_products.append(detailed_product)
                analyzed_products.add(product_id)
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                time.sleep(1)
        
        if new_products:
            # æŒ‰æŠ•ç¥¨æ•°æ’åº
            new_products.sort(key=lambda x: x.get('votes', 0), reverse=True)
            
            # ç”Ÿæˆæ–‡ç« 
            success = self.generate_article(new_products)
            
            if success:
                # ä¿å­˜å†å²è®°å½•
                self.save_analyzed_products(analyzed_products)
                print(f"ğŸ‰ åˆ†æå®Œæˆï¼å…±åˆ†æ {len(new_products)} ä¸ªäº§å“")
                return True
        
        print("ğŸ“ ä»Šæ—¥æ— æ–°äº§å“éœ€è¦åˆ†æ")
        return False


def main():
    """ä¸»å‡½æ•°"""
    analyzer = ProductHuntAnalyzer()
    success = analyzer.run_analysis(max_products=3)
    
    if not success:
        print("âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜")
        exit(1)
    
    print("âœ… Product Hunt TOP3äº§å“åˆ†æå®Œæˆ")


if __name__ == "__main__":
    main()