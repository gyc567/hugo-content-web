#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨

è¯¥è„šæœ¬æ‰§è¡Œæ‰€æœ‰æµ‹è¯•ç”¨ä¾‹å¹¶ç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Šï¼ŒåŒ…æ‹¬ï¼š
- å•å…ƒæµ‹è¯•è¦†ç›–ç‡
- é›†æˆæµ‹è¯•ç»“æœ
- æ€§èƒ½æµ‹è¯•æ•°æ®
- ä»£ç è´¨é‡æ£€æŸ¥

ä½œè€…: Qoder AI Assistant
åˆ›å»ºæ—¶é—´: 2025-08-24
"""

import unittest
import sys
import os
import time
import datetime
import json
import subprocess
from io import StringIO
from contextlib import redirect_stdout, redirect_stderr
import traceback


class TestReportGenerator:
    """æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨"""
        self.results = {
            'timestamp': datetime.datetime.now().isoformat(),
            'summary': {},
            'unit_tests': {},
            'integration_tests': {},
            'performance_tests': {},
            'code_quality': {},
            'coverage': {},
            'recommendations': []
        }
        
        # è®¾ç½®æµ‹è¯•ç¯å¢ƒ
        os.environ['TESTING'] = '1'
        sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
    
    def run_unit_tests(self) -> dict:
        """è¿è¡Œå•å…ƒæµ‹è¯•"""
        print("ğŸ§ª è¿è¡Œå•å…ƒæµ‹è¯•...")
        
        try:
            # å¯¼å…¥æµ‹è¯•æ¨¡å—
            from test_project_deduplicator import (
                TestProjectDeduplicator,
                TestProjectDeduplicatorIntegration,
                TestProjectDeduplicatorPerformance
            )
            
            # åˆ›å»ºæµ‹è¯•å¥—ä»¶
            loader = unittest.TestLoader()
            suite = unittest.TestSuite()
            
            # æ·»åŠ æµ‹è¯•ç±»
            suite.addTests(loader.loadTestsFromTestCase(TestProjectDeduplicator))
            suite.addTests(loader.loadTestsFromTestCase(TestProjectDeduplicatorIntegration))
            suite.addTests(loader.loadTestsFromTestCase(TestProjectDeduplicatorPerformance))
            
            # è¿è¡Œæµ‹è¯•
            stream = StringIO()
            runner = unittest.TextTestRunner(stream=stream, verbosity=2)
            result = runner.run(suite)
            
            # æ”¶é›†ç»“æœ
            unit_results = {
                'total_tests': result.testsRun,
                'failures': len(result.failures),
                'errors': len(result.errors),
                'skipped': len(result.skipped) if hasattr(result, 'skipped') else 0,
                'success_rate': ((result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun * 100) if result.testsRun > 0 else 0,
                'output': stream.getvalue(),
                'failure_details': [{'test': str(test), 'error': error} for test, error in result.failures],
                'error_details': [{'test': str(test), 'error': error} for test, error in result.errors]
            }
            
            print(f"   âœ… å•å…ƒæµ‹è¯•å®Œæˆ: {unit_results['total_tests']} ä¸ªæµ‹è¯•ï¼ŒæˆåŠŸç‡ {unit_results['success_rate']:.1f}%")
            return unit_results
            
        except Exception as e:
            print(f"   âŒ å•å…ƒæµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            return {
                'total_tests': 0,
                'failures': 1,
                'errors': 0,
                'skipped': 0,
                'success_rate': 0,
                'output': f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {str(e)}",
                'failure_details': [],
                'error_details': [{'test': 'unit_test_execution', 'error': str(e)}]
            }
    
    def run_integration_tests(self) -> dict:
        """è¿è¡Œé›†æˆæµ‹è¯•"""
        print("ğŸ”— è¿è¡Œé›†æˆæµ‹è¯•...")
        
        try:
            # åŠ¨æ€å¯¼å…¥é›†æˆæµ‹è¯•æ¨¡å—
            import importlib.util
            spec = importlib.util.spec_from_file_location("test_integration", "test_integration.py")
            test_integration_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(test_integration_module)
            
            TestClaudeAgentAnalyzerIntegration = test_integration_module.TestClaudeAgentAnalyzerIntegration
            TestDataMigrationIntegration = test_integration_module.TestDataMigrationIntegration
            
            # åˆ›å»ºæµ‹è¯•å¥—ä»¶
            loader = unittest.TestLoader()
            suite = unittest.TestSuite()
            
            # æ·»åŠ é›†æˆæµ‹è¯•
            suite.addTests(loader.loadTestsFromTestCase(TestClaudeAgentAnalyzerIntegration))
            suite.addTests(loader.loadTestsFromTestCase(TestDataMigrationIntegration))
            
            # è¿è¡Œæµ‹è¯•
            stream = StringIO()
            runner = unittest.TextTestRunner(stream=stream, verbosity=2)
            result = runner.run(suite)
            
            # æ”¶é›†ç»“æœ
            integration_results = {
                'total_tests': result.testsRun,
                'failures': len(result.failures),
                'errors': len(result.errors),
                'skipped': len(result.skipped) if hasattr(result, 'skipped') else 0,
                'success_rate': ((result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun * 100) if result.testsRun > 0 else 0,
                'output': stream.getvalue(),
                'failure_details': [{'test': str(test), 'error': error} for test, error in result.failures],
                'error_details': [{'test': str(test), 'error': error} for test, error in result.errors]
            }
            
            print(f"   âœ… é›†æˆæµ‹è¯•å®Œæˆ: {integration_results['total_tests']} ä¸ªæµ‹è¯•ï¼ŒæˆåŠŸç‡ {integration_results['success_rate']:.1f}%")
            return integration_results
            
        except Exception as e:
            print(f"   âŒ é›†æˆæµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            return {
                'total_tests': 0,
                'failures': 1,
                'errors': 0,
                'skipped': 0,
                'success_rate': 0,
                'output': f"é›†æˆæµ‹è¯•æ‰§è¡Œå¤±è´¥: {str(e)}",
                'failure_details': [],
                'error_details': [{'test': 'integration_test_execution', 'error': str(e)}]
            }
    
    def run_performance_tests(self) -> dict:
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        print("âš¡ è¿è¡Œæ€§èƒ½æµ‹è¯•...")
        
        try:
            from project_deduplicator import ProjectDeduplicator
            import tempfile
            import shutil
            
            # åˆ›å»ºä¸´æ—¶æµ‹è¯•ç¯å¢ƒ
            temp_dir = tempfile.mkdtemp()
            test_file = os.path.join(temp_dir, 'perf_test.json')
            
            try:
                deduplicator = ProjectDeduplicator(test_file)
                
                # æ€§èƒ½æµ‹è¯•æŒ‡æ ‡
                performance_metrics = {}
                
                # æµ‹è¯•1: å¤§é‡é¡¹ç›®æ·»åŠ æ€§èƒ½
                print("   ğŸ“Š æµ‹è¯•é¡¹ç›®æ·»åŠ æ€§èƒ½...")
                start_time = time.time()
                
                for i in range(1000):
                    project = {
                        "full_name": f"user{i}/project{i}",
                        "html_url": f"https://github.com/user{i}/project{i}",
                        "stargazers_count": i
                    }
                    deduplicator.add_analyzed_project(project)
                
                add_time = time.time() - start_time
                performance_metrics['add_1000_projects_time'] = add_time
                performance_metrics['avg_add_time'] = add_time / 1000
                
                # æµ‹è¯•2: å¤§é‡é¡¹ç›®æŸ¥è¯¢æ€§èƒ½
                print("   ğŸ” æµ‹è¯•é¡¹ç›®æŸ¥è¯¢æ€§èƒ½...")
                start_time = time.time()
                
                for i in range(500):
                    project = {
                        "full_name": f"user{i}/project{i}",
                        "html_url": f"https://github.com/user{i}/project{i}"
                    }
                    deduplicator.is_duplicate_project(project)
                
                query_time = time.time() - start_time
                performance_metrics['query_500_projects_time'] = query_time
                performance_metrics['avg_query_time'] = query_time / 500
                
                # æµ‹è¯•3: å†…å­˜ä½¿ç”¨
                import psutil
                process = psutil.Process(os.getpid())
                memory_info = process.memory_info()
                performance_metrics['memory_usage_mb'] = memory_info.rss / 1024 / 1024
                
                # æµ‹è¯•4: æ–‡ä»¶å¤§å°
                if os.path.exists(test_file):
                    file_size = os.path.getsize(test_file)
                    performance_metrics['data_file_size_kb'] = file_size / 1024
                
                # æ€§èƒ½è¯„ä¼°
                performance_score = 100
                if performance_metrics['avg_add_time'] > 0.001:  # è¶…è¿‡1ms
                    performance_score -= 20
                if performance_metrics['avg_query_time'] > 0.001:  # è¶…è¿‡1ms
                    performance_score -= 20
                if performance_metrics['memory_usage_mb'] > 50:  # è¶…è¿‡50MB
                    performance_score -= 10
                
                performance_metrics['performance_score'] = max(0, performance_score)
                
                print(f"   âœ… æ€§èƒ½æµ‹è¯•å®Œæˆ: è¯„åˆ† {performance_metrics['performance_score']}/100")
                
                return {
                    'status': 'success',
                    'metrics': performance_metrics,
                    'recommendations': self._generate_performance_recommendations(performance_metrics)
                }
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                shutil.rmtree(temp_dir)
                
        except Exception as e:
            print(f"   âŒ æ€§èƒ½æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            return {
                'status': 'failed',
                'error': str(e),
                'metrics': {},
                'recommendations': ['æ€§èƒ½æµ‹è¯•æ‰§è¡Œå¤±è´¥ï¼Œå»ºè®®æ£€æŸ¥æµ‹è¯•ç¯å¢ƒ']
            }
    
    def _generate_performance_recommendations(self, metrics: dict) -> list:
        """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        if metrics.get('avg_add_time', 0) > 0.001:
            recommendations.append("é¡¹ç›®æ·»åŠ æ“ä½œè€—æ—¶è¾ƒé•¿ï¼Œå»ºè®®ä¼˜åŒ–æ•°æ®å†™å…¥é€»è¾‘")
        
        if metrics.get('avg_query_time', 0) > 0.001:
            recommendations.append("é¡¹ç›®æŸ¥è¯¢æ“ä½œè€—æ—¶è¾ƒé•¿ï¼Œå»ºè®®ä½¿ç”¨ç´¢å¼•æˆ–ç¼“å­˜ä¼˜åŒ–")
        
        if metrics.get('memory_usage_mb', 0) > 50:
            recommendations.append("å†…å­˜ä½¿ç”¨è¾ƒé«˜ï¼Œå»ºè®®ä¼˜åŒ–æ•°æ®ç»“æ„")
        
        if metrics.get('data_file_size_kb', 0) > 1000:
            recommendations.append("æ•°æ®æ–‡ä»¶è¾ƒå¤§ï¼Œå»ºè®®è€ƒè™‘æ•°æ®å‹ç¼©æˆ–åˆ†ç‰‡å­˜å‚¨")
        
        if not recommendations:
            recommendations.append("æ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œæ— éœ€ä¼˜åŒ–")
        
        return recommendations
    
    def check_code_quality(self) -> dict:
        """æ£€æŸ¥ä»£ç è´¨é‡"""
        print("ğŸ” æ£€æŸ¥ä»£ç è´¨é‡...")
        
        quality_results = {
            'syntax_check': {},
            'import_check': {},
            'style_check': {},
            'complexity_check': {}
        }
        
        # æ£€æŸ¥çš„æ–‡ä»¶åˆ—è¡¨
        files_to_check = [
            'project_deduplicator.py',
            'crypto-project-analyzer.py',
            'migrate_data.py'
        ]
        
        for filename in files_to_check:
            print(f"   ğŸ“„ æ£€æŸ¥æ–‡ä»¶: {filename}")
            
            if not os.path.exists(filename):
                quality_results['syntax_check'][filename] = {'status': 'missing', 'errors': []}
                continue
            
            # è¯­æ³•æ£€æŸ¥
            try:
                with open(filename, 'r', encoding='utf-8') as f:
                    code = f.read()
                
                compile(code, filename, 'exec')
                quality_results['syntax_check'][filename] = {'status': 'pass', 'errors': []}
            except SyntaxError as e:
                quality_results['syntax_check'][filename] = {
                    'status': 'fail',
                    'errors': [f"è¯­æ³•é”™è¯¯: {e}"]
                }
            except Exception as e:
                quality_results['syntax_check'][filename] = {
                    'status': 'error',
                    'errors': [f"æ£€æŸ¥å¤±è´¥: {e}"]
                }
        
        # è®¡ç®—æ•´ä½“è´¨é‡è¯„åˆ†
        total_files = len(files_to_check)
        passed_files = sum(1 for result in quality_results['syntax_check'].values() 
                          if result['status'] == 'pass')
        
        quality_score = (passed_files / total_files * 100) if total_files > 0 else 0
        quality_results['overall_score'] = quality_score
        
        print(f"   âœ… ä»£ç è´¨é‡æ£€æŸ¥å®Œæˆ: è¯„åˆ† {quality_score:.1f}/100")
        
        return quality_results
    
    def calculate_test_coverage(self) -> dict:
        """è®¡ç®—æµ‹è¯•è¦†ç›–ç‡"""
        print("ğŸ“Š è®¡ç®—æµ‹è¯•è¦†ç›–ç‡...")
        
        try:
            # ç»Ÿè®¡æºä»£ç è¡Œæ•°
            source_files = [
                'project_deduplicator.py',
                'migrate_data.py'
            ]
            
            total_lines = 0
            for filename in source_files:
                if os.path.exists(filename):
                    with open(filename, 'r', encoding='utf-8') as f:
                        lines = len([line for line in f if line.strip() and not line.strip().startswith('#')])
                        total_lines += lines
            
            # ç»Ÿè®¡æµ‹è¯•ä»£ç è¡Œæ•°
            test_files = [
                'test_project_deduplicator.py',
                'test_integration.py'
            ]
            
            test_lines = 0
            for filename in test_files:
                if os.path.exists(filename):
                    with open(filename, 'r', encoding='utf-8') as f:
                        lines = len([line for line in f if line.strip() and not line.strip().startswith('#')])
                        test_lines += lines
            
            # ä¼°ç®—è¦†ç›–ç‡ï¼ˆåŸºäºæµ‹è¯•ä»£ç é‡ï¼‰
            estimated_coverage = min(100, (test_lines / total_lines * 50)) if total_lines > 0 else 0
            
            coverage_result = {
                'source_lines': total_lines,
                'test_lines': test_lines,
                'estimated_coverage': estimated_coverage,
                'status': 'estimated'  # è¡¨ç¤ºè¿™æ˜¯ä¼°ç®—å€¼
            }
            
            print(f"   âœ… æµ‹è¯•è¦†ç›–ç‡ä¼°ç®—å®Œæˆ: {estimated_coverage:.1f}%")
            
            return coverage_result
            
        except Exception as e:
            print(f"   âŒ è¦†ç›–ç‡è®¡ç®—å¤±è´¥: {e}")
            return {
                'source_lines': 0,
                'test_lines': 0,
                'estimated_coverage': 0,
                'status': 'failed',
                'error': str(e)
            }
    
    def generate_recommendations(self) -> list:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        # åŸºäºæµ‹è¯•ç»“æœç”Ÿæˆå»ºè®®
        unit_success_rate = self.results['unit_tests'].get('success_rate', 0)
        integration_success_rate = self.results['integration_tests'].get('success_rate', 0)
        performance_score = self.results['performance_tests'].get('metrics', {}).get('performance_score', 0)
        quality_score = self.results['code_quality'].get('overall_score', 0)
        coverage = self.results['coverage'].get('estimated_coverage', 0)
        
        if unit_success_rate < 100:
            recommendations.append("å•å…ƒæµ‹è¯•å­˜åœ¨å¤±è´¥ç”¨ä¾‹ï¼Œå»ºè®®ä¿®å¤å¤±è´¥çš„æµ‹è¯•")
        
        if integration_success_rate < 100:
            recommendations.append("é›†æˆæµ‹è¯•å­˜åœ¨é—®é¢˜ï¼Œå»ºè®®æ£€æŸ¥ç³»ç»Ÿé›†æˆé€»è¾‘")
        
        if performance_score < 80:
            recommendations.append("æ€§èƒ½è¡¨ç°æœ‰å¾…æå‡ï¼Œå»ºè®®è¿›è¡Œæ€§èƒ½ä¼˜åŒ–")
        
        if quality_score < 90:
            recommendations.append("ä»£ç è´¨é‡éœ€è¦æ”¹è¿›ï¼Œå»ºè®®ä¿®å¤è¯­æ³•é”™è¯¯å’Œæ”¹è¿›ä»£ç é£æ ¼")
        
        if coverage < 80:
            recommendations.append("æµ‹è¯•è¦†ç›–ç‡åä½ï¼Œå»ºè®®å¢åŠ æµ‹è¯•ç”¨ä¾‹")
        
        # æ·»åŠ å…·ä½“çš„ä¼˜åŒ–å»ºè®®
        if self.results['performance_tests'].get('recommendations'):
            recommendations.extend(self.results['performance_tests']['recommendations'])
        
        if not recommendations:
            recommendations.append("æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Œä»£ç è´¨é‡è‰¯å¥½ï¼å»ºè®®ç»§ç»­ä¿æŒã€‚")
        
        return recommendations
    
    def generate_summary(self) -> dict:
        """ç”Ÿæˆæµ‹è¯•æ‘˜è¦"""
        summary = {
            'overall_status': 'PASS',
            'total_tests': 0,
            'total_failures': 0,
            'total_errors': 0,
            'success_rate': 0,
            'performance_score': 0,
            'quality_score': 0,
            'coverage': 0
        }
        
        # æ±‡æ€»æµ‹è¯•æ•°æ®
        for test_type in ['unit_tests', 'integration_tests']:
            if test_type in self.results:
                summary['total_tests'] += self.results[test_type].get('total_tests', 0)
                summary['total_failures'] += self.results[test_type].get('failures', 0)
                summary['total_errors'] += self.results[test_type].get('errors', 0)
        
        # è®¡ç®—æ•´ä½“æˆåŠŸç‡
        if summary['total_tests'] > 0:
            passed_tests = summary['total_tests'] - summary['total_failures'] - summary['total_errors']
            summary['success_rate'] = (passed_tests / summary['total_tests']) * 100
        
        # è·å–å…¶ä»–æŒ‡æ ‡
        summary['performance_score'] = self.results['performance_tests'].get('metrics', {}).get('performance_score', 0)
        summary['quality_score'] = self.results['code_quality'].get('overall_score', 0)
        summary['coverage'] = self.results['coverage'].get('estimated_coverage', 0)
        
        # åˆ¤æ–­æ•´ä½“çŠ¶æ€
        if summary['total_failures'] > 0 or summary['total_errors'] > 0:
            summary['overall_status'] = 'FAIL'
        elif summary['success_rate'] < 80:
            summary['overall_status'] = 'WARNING'
        
        return summary
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹æ‰§è¡Œé¡¹ç›®å»é‡ä¼˜åŒ–å…¨é¢æµ‹è¯•...")
        print("=" * 80)
        
        # è¿è¡Œå„ç±»æµ‹è¯•
        self.results['unit_tests'] = self.run_unit_tests()
        self.results['integration_tests'] = self.run_integration_tests()
        self.results['performance_tests'] = self.run_performance_tests()
        self.results['code_quality'] = self.check_code_quality()
        self.results['coverage'] = self.calculate_test_coverage()
        
        # ç”Ÿæˆæ‘˜è¦å’Œå»ºè®®
        self.results['summary'] = self.generate_summary()
        self.results['recommendations'] = self.generate_recommendations()
        
        print("\n" + "=" * 80)
        print("ğŸ“‹ æµ‹è¯•å®Œæˆï¼Œæ­£åœ¨ç”ŸæˆæŠ¥å‘Š...")
    
    def save_report(self, filename: str = None):
        """ä¿å­˜æµ‹è¯•æŠ¥å‘Š"""
        if filename is None:
            timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'test_report_{timestamp}.json'
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“„ è¯¦ç»†æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {filename}")
            return filename
        except Exception as e:
            print(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
            return None
    
    def print_summary_report(self):
        """æ‰“å°æ‘˜è¦æŠ¥å‘Š"""
        summary = self.results['summary']
        
        print("\n" + "ğŸ¯ æµ‹è¯•æ€»ç»“æŠ¥å‘Š".center(80, "="))
        print(f"\nğŸ“Š æ•´ä½“çŠ¶æ€: {'âœ… ' + summary['overall_status'] if summary['overall_status'] == 'PASS' else 'âŒ ' + summary['overall_status']}")
        print(f"ğŸ“ˆ æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
        print(f"âœ… æˆåŠŸç‡: {summary['success_rate']:.1f}%")
        print(f"âŒ å¤±è´¥æ•°: {summary['total_failures']}")
        print(f"ğŸ’¥ é”™è¯¯æ•°: {summary['total_errors']}")
        print(f"âš¡ æ€§èƒ½è¯„åˆ†: {summary['performance_score']:.1f}/100")
        print(f"ğŸ” ä»£ç è´¨é‡: {summary['quality_score']:.1f}/100")
        print(f"ğŸ“Š æµ‹è¯•è¦†ç›–ç‡: {summary['coverage']:.1f}%")
        
        print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        for i, recommendation in enumerate(self.results['recommendations'], 1):
            print(f"   {i}. {recommendation}")
        
        print("\n" + "=" * 80)


def main():
    """ä¸»å‡½æ•°"""
    generator = TestReportGenerator()
    
    try:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        generator.run_all_tests()
        
        # æ‰“å°æ‘˜è¦æŠ¥å‘Š
        generator.print_summary_report()
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = generator.save_report()
        
        # è¿”å›é€€å‡ºç 
        summary = generator.results['summary']
        if summary['overall_status'] == 'PASS':
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
            sys.exit(0)
        else:
            print("âš ï¸  å­˜åœ¨æµ‹è¯•é—®é¢˜ï¼Œè¯·æŸ¥çœ‹æŠ¥å‘Šè¯¦æƒ…ã€‚")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\nâŒ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(2)
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {e}")
        traceback.print_exc()
        sys.exit(3)


if __name__ == "__main__":
    main()