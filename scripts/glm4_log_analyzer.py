#!/usr/bin/env python3
"""
GLM-4.5 æ—¥å¿—åˆ†æå·¥å…·
SuperCopyCoder - æ¨¡ä»¿ï¼Œæ˜¯æœ€å¥½çš„è‡´æ•¬ã€‚ç«™åœ¨å·¨äººçš„è‚©è†€ï¼Œç«™å¾—æ›´é«˜

åˆ†æGLM-4.5 APIè°ƒç”¨æ—¥å¿—ï¼Œç”Ÿæˆç»Ÿè®¡æŠ¥å‘Šå’Œä½¿ç”¨åˆ†æ
"""

import os
import json
import re
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import argparse


class GLM4LogAnalyzer:
    """GLM-4.5æ—¥å¿—åˆ†æå™¨"""
    
    def __init__(self, log_dir: Optional[str] = None):
        """
        åˆå§‹åŒ–æ—¥å¿—åˆ†æå™¨
        
        Args:
            log_dir: æ—¥å¿—ç›®å½•è·¯å¾„
        """
        self.script_dir = Path(__file__).parent
        self.log_dir = Path(log_dir) if log_dir else self.script_dir / 'logs'
        
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        self.log_dir.mkdir(exist_ok=True)
        
        # æ—¥å¿—æ–‡ä»¶è·¯å¾„
        self.main_log = self.log_dir / 'glm4_client.log'
        self.request_log = self.log_dir / 'glm4_requests.log'
        
        # ç»Ÿè®¡æ•°æ®
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_tokens': 0,
            'input_tokens': 0,
            'output_tokens': 0,
            'models_used': {},
            'daily_usage': {},
            'hourly_usage': {},
            'average_response_time': 0,
            'token_costs': {},
            'error_types': {}
        }
    
    def analyze_logs(self, start_date: Optional[str] = None, end_date: Optional[str] = None) -> Dict[str, Any]:
        """
        åˆ†ææ—¥å¿—æ–‡ä»¶
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        print(f"å¼€å§‹åˆ†ææ—¥å¿—... æ—¥å¿—ç›®å½•: {self.log_dir}")
        
        # é‡ç½®ç»Ÿè®¡æ•°æ®
        self._reset_stats()
        
        # è§£ææ—¥æœŸèŒƒå›´
        date_filter = self._parse_date_range(start_date, end_date)
        
        # åˆ†æè¯·æ±‚æ—¥å¿—
        if self.request_log.exists():
            self._analyze_request_log(date_filter)
        else:
            print(f"è¯·æ±‚æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {self.request_log}")
        
        # åˆ†æä¸»æ—¥å¿—
        if self.main_log.exists():
            self._analyze_main_log(date_filter)
        else:
            print(f"ä¸»æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {self.main_log}")
        
        # è®¡ç®—æ´¾ç”Ÿç»Ÿè®¡æ•°æ®
        self._calculate_derived_stats()
        
        return self.stats
    
    def _reset_stats(self):
        """é‡ç½®ç»Ÿè®¡æ•°æ®"""
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_tokens': 0,
            'input_tokens': 0,
            'output_tokens': 0,
            'models_used': {},
            'daily_usage': {},
            'hourly_usage': {},
            'average_response_time': 0,
            'token_costs': {},
            'error_types': {},
            'request_details': []
        }
    
    def _parse_date_range(self, start_date: Optional[str], end_date: Optional[str]) -> Optional[Tuple[datetime, datetime]]:
        """è§£ææ—¥æœŸèŒƒå›´"""
        if not start_date and not end_date:
            return None
        
        try:
            start = datetime.strptime(start_date, '%Y-%m-%d') if start_date else datetime.min
            end = datetime.strptime(end_date, '%Y-%m-%d') + timedelta(days=1) if end_date else datetime.max
            return (start, end)
        except ValueError as e:
            print(f"æ—¥æœŸæ ¼å¼é”™è¯¯: {e}")
            return None
    
    def _analyze_request_log(self, date_filter: Optional[Tuple[datetime, datetime]]):
        """åˆ†æè¯·æ±‚æ—¥å¿—"""
        print("åˆ†æè¯·æ±‚æ—¥å¿—...")
        
        try:
            with open(self.request_log, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    try:
                        self._process_request_log_line(line, date_filter)
                    except Exception as e:
                        print(f"å¤„ç†è¯·æ±‚æ—¥å¿—ç¬¬{line_num}è¡Œæ—¶å‡ºé”™: {e}")
                        
        except Exception as e:
            print(f"è¯»å–è¯·æ±‚æ—¥å¿—å¤±è´¥: {e}")
    
    def _process_request_log_line(self, line: str, date_filter: Optional[Tuple[datetime, datetime]]):
        """å¤„ç†å•è¡Œè¯·æ±‚æ—¥å¿—"""
        line = line.strip()
        if not line:
            return
        
        # è§£ææ—¥å¿—è¡Œ
        parts = line.split(' | ', 2)
        if len(parts) < 3:
            return
        
        timestamp_str, log_type, content = parts
        
        # è§£ææ—¶é—´æˆ³
        try:
            timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')
        except ValueError:
            return
        
        # åº”ç”¨æ—¥æœŸè¿‡æ»¤å™¨
        if date_filter:
            start_date, end_date = date_filter
            if not (start_date <= timestamp < end_date):
                return
        
        # å¤„ç†è¯·æ±‚æˆ–å“åº”
        if log_type == 'REQUEST':
            self._process_request_entry(timestamp, content)
        elif log_type == 'RESPONSE':
            self._process_response_entry(timestamp, content)
    
    def _process_request_entry(self, timestamp: datetime, content: str):
        """å¤„ç†è¯·æ±‚æ¡ç›®"""
        try:
            if content.startswith('REQUEST | '):
                json_str = content[10:]  # å»æ‰ 'REQUEST | ' å‰ç¼€
                data = json.loads(json_str)
                
                # è®°å½•è¯·æ±‚ä¿¡æ¯
                self.stats['total_requests'] += 1
                
                # æŒ‰æ—¥æœŸç»Ÿè®¡
                date_key = timestamp.strftime('%Y-%m-%d')
                self.stats['daily_usage'][date_key] = self.stats['daily_usage'].get(date_key, 0) + 1
                
                # æŒ‰å°æ—¶ç»Ÿè®¡
                hour_key = timestamp.strftime('%H')
                self.stats['hourly_usage'][hour_key] = self.stats['hourly_usage'].get(hour_key, 0) + 1
                
                # æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡
                model = data.get('model', 'unknown')
                self.stats['models_used'][model] = self.stats['models_used'].get(model, 0) + 1
                
        except (json.JSONDecodeError, KeyError) as e:
            # å¿½ç•¥è§£æé”™è¯¯
            pass
    
    def _process_response_entry(self, timestamp: datetime, content: str):
        """å¤„ç†å“åº”æ¡ç›®"""
        try:
            if content.startswith('RESPONSE | '):
                json_str = content[11:]  # å»æ‰ 'RESPONSE | ' å‰ç¼€
                data = json.loads(json_str)
                
                # æˆåŠŸè¯·æ±‚è®¡æ•°
                self.stats['successful_requests'] += 1
                
                # Tokenä½¿ç”¨ç»Ÿè®¡
                token_usage = data.get('token_usage', {})
                self.stats['input_tokens'] += token_usage.get('prompt_tokens', 0)
                self.stats['output_tokens'] += token_usage.get('completion_tokens', 0)
                self.stats['total_tokens'] += token_usage.get('total_tokens', 0)
                
                # ä¿å­˜è¯·æ±‚è¯¦æƒ…
                self.stats['request_details'].append({
                    'timestamp': timestamp.isoformat(),
                    'request_id': data.get('request_id'),
                    'model': data.get('model'),
                    'token_usage': token_usage,
                    'choices_count': data.get('choices_count', 0)
                })
                
        except (json.JSONDecodeError, KeyError):
            # å¿½ç•¥è§£æé”™è¯¯
            pass
    
    def _analyze_main_log(self, date_filter: Optional[Tuple[datetime, datetime]]):
        """åˆ†æä¸»æ—¥å¿—"""
        print("åˆ†æä¸»æ—¥å¿—...")
        
        try:
            with open(self.main_log, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    try:
                        self._process_main_log_line(line, date_filter)
                    except Exception as e:
                        print(f"å¤„ç†ä¸»æ—¥å¿—ç¬¬{line_num}è¡Œæ—¶å‡ºé”™: {e}")
                        
        except Exception as e:
            print(f"è¯»å–ä¸»æ—¥å¿—å¤±è´¥: {e}")
    
    def _process_main_log_line(self, line: str, date_filter: Optional[Tuple[datetime, datetime]]):
        """å¤„ç†å•è¡Œä¸»æ—¥å¿—"""
        line = line.strip()
        if not line:
            return
        
        # è§£æé”™è¯¯æ—¥å¿—
        if 'ERROR' in line and 'GLM-4.5 APIè¯·æ±‚å¤±è´¥' in line:
            self.stats['failed_requests'] += 1
            
            # æå–é”™è¯¯ç±»å‹
            error_match = re.search(r'Error: (.+?)(?:\s|$)', line)
            if error_match:
                error_type = error_match.group(1)
                self.stats['error_types'][error_type] = self.stats['error_types'].get(error_type, 0) + 1
    
    def _calculate_derived_stats(self):
        """è®¡ç®—æ´¾ç”Ÿç»Ÿè®¡æ•°æ®"""
        # è®¡ç®—æˆåŠŸç‡
        if self.stats['total_requests'] > 0:
            self.stats['success_rate'] = (self.stats['successful_requests'] / self.stats['total_requests']) * 100
        else:
            self.stats['success_rate'] = 0
        
        # è®¡ç®—å¹³å‡tokenä½¿ç”¨
        if self.stats['successful_requests'] > 0:
            self.stats['avg_input_tokens'] = self.stats['input_tokens'] / self.stats['successful_requests']
            self.stats['avg_output_tokens'] = self.stats['output_tokens'] / self.stats['successful_requests']
            self.stats['avg_total_tokens'] = self.stats['total_tokens'] / self.stats['successful_requests']
        else:
            self.stats['avg_input_tokens'] = 0
            self.stats['avg_output_tokens'] = 0
            self.stats['avg_total_tokens'] = 0
    
    def generate_report(self, output_file: Optional[str] = None) -> str:
        """
        ç”Ÿæˆåˆ†ææŠ¥å‘Š
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            æŠ¥å‘Šå†…å®¹
        """
        report_lines = []
        
        # æ ‡é¢˜
        report_lines.append("GLM-4.5 APIä½¿ç”¨åˆ†ææŠ¥å‘Š")
        report_lines.append("=" * 50)
        report_lines.append(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append("")
        
        # åŸºæœ¬ç»Ÿè®¡
        report_lines.append("ğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
        report_lines.append(f"  æ€»è¯·æ±‚æ•°: {self.stats['total_requests']:,}")
        report_lines.append(f"  æˆåŠŸè¯·æ±‚: {self.stats['successful_requests']:,}")
        report_lines.append(f"  å¤±è´¥è¯·æ±‚: {self.stats['failed_requests']:,}")
        report_lines.append(f"  æˆåŠŸç‡: {self.stats['success_rate']:.2f}%")
        report_lines.append("")
        
        # Tokenä½¿ç”¨ç»Ÿè®¡
        report_lines.append("ğŸ¯ Tokenä½¿ç”¨ç»Ÿè®¡:")
        report_lines.append(f"  æ€»Tokenæ¶ˆè€—: {self.stats['total_tokens']:,}")
        report_lines.append(f"  è¾“å…¥Token: {self.stats['input_tokens']:,}")
        report_lines.append(f"  è¾“å‡ºToken: {self.stats['output_tokens']:,}")
        report_lines.append(f"  å¹³å‡è¾“å…¥Token: {self.stats['avg_input_tokens']:.1f}")
        report_lines.append(f"  å¹³å‡è¾“å‡ºToken: {self.stats['avg_output_tokens']:.1f}")
        report_lines.append(f"  å¹³å‡æ€»Token: {self.stats['avg_total_tokens']:.1f}")
        report_lines.append("")
        
        # æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡
        if self.stats['models_used']:
            report_lines.append("ğŸ¤– æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡:")
            for model, count in sorted(self.stats['models_used'].items(), key=lambda x: x[1], reverse=True):
                percentage = (count / self.stats['total_requests']) * 100
                report_lines.append(f"  {model}: {count:,} ({percentage:.1f}%)")
            report_lines.append("")
        
        # æ¯æ—¥ä½¿ç”¨ç»Ÿè®¡
        if self.stats['daily_usage']:
            report_lines.append("ğŸ“… æ¯æ—¥ä½¿ç”¨ç»Ÿè®¡:")
            for date, count in sorted(self.stats['daily_usage'].items()):
                report_lines.append(f"  {date}: {count:,} è¯·æ±‚")
            report_lines.append("")
        
        # æ¯å°æ—¶ä½¿ç”¨ç»Ÿè®¡
        if self.stats['hourly_usage']:
            report_lines.append("ğŸ• æ¯å°æ—¶ä½¿ç”¨ç»Ÿè®¡:")
            for hour in sorted(self.stats['hourly_usage'].keys(), key=int):
                count = self.stats['hourly_usage'][hour]
                report_lines.append(f"  {hour}:00: {count:,} è¯·æ±‚")
            report_lines.append("")
        
        # é”™è¯¯ç»Ÿè®¡
        if self.stats['error_types']:
            report_lines.append("âŒ é”™è¯¯ç»Ÿè®¡:")
            for error, count in sorted(self.stats['error_types'].items(), key=lambda x: x[1], reverse=True):
                report_lines.append(f"  {error}: {count:,} æ¬¡")
            report_lines.append("")
        
        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        report_content = "\n".join(report_lines)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        if output_file:
            try:
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(report_content)
                print(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
            except Exception as e:
                print(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
        
        return report_content
    
    def export_stats_json(self, output_file: Optional[str] = None):
        """å¯¼å‡ºç»Ÿè®¡æ•°æ®ä¸ºJSON"""
        if not output_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = self.log_dir / f"glm4_analysis_{timestamp}.json"
        
        analysis_data = {
            'timestamp': datetime.now().isoformat(),
            'log_directory': str(self.log_dir),
            'stats': self.stats,
            'summary': {
                'total_requests': self.stats['total_requests'],
                'successful_requests': self.stats['successful_requests'],
                'success_rate': self.stats['success_rate'],
                'total_tokens': self.stats['total_tokens'],
                'most_used_model': max(self.stats['models_used'].items(), key=lambda x: x[1])[0] if self.stats['models_used'] else None
            }
        }
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(analysis_data, f, ensure_ascii=False, indent=2)
            print(f"ç»Ÿè®¡æ•°æ®å·²å¯¼å‡ºåˆ°: {output_file}")
        except Exception as e:
            print(f"å¯¼å‡ºç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")
    
    def clean_old_logs(self, days: int = 30):
        """æ¸…ç†æ—§æ—¥å¿—æ–‡ä»¶"""
        cutoff_date = datetime.now() - timedelta(days=days)
        cleaned_count = 0
        
        for log_file in self.log_dir.glob('*.log*'):
            try:
                file_mtime = datetime.fromtimestamp(log_file.stat().st_mtime)
                if file_mtime < cutoff_date:
                    log_file.unlink()
                    cleaned_count += 1
                    print(f"å·²åˆ é™¤æ—§æ—¥å¿—æ–‡ä»¶: {log_file}")
            except Exception as e:
                print(f"åˆ é™¤æ—¥å¿—æ–‡ä»¶å¤±è´¥ {log_file}: {e}")
        
        print(f"æ¸…ç†å®Œæˆï¼Œå…±åˆ é™¤ {cleaned_count} ä¸ªæ—§æ—¥å¿—æ–‡ä»¶")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='GLM-4.5æ—¥å¿—åˆ†æå·¥å…·')
    parser.add_argument('--log-dir', help='æ—¥å¿—ç›®å½•è·¯å¾„')
    parser.add_argument('--start-date', help='å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--end-date', help='ç»“æŸæ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--output', help='æŠ¥å‘Šè¾“å‡ºæ–‡ä»¶')
    parser.add_argument('--export-json', help='å¯¼å‡ºJSONç»Ÿè®¡æ–‡ä»¶')
    parser.add_argument('--clean-logs', type=int, metavar='DAYS', help='æ¸…ç†Nå¤©å‰çš„æ—§æ—¥å¿—')
    
    args = parser.parse_args()
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = GLM4LogAnalyzer(args.log_dir)
    
    # æ¸…ç†æ—§æ—¥å¿—
    if args.clean_logs:
        analyzer.clean_old_logs(args.clean_logs)
        return
    
    # åˆ†ææ—¥å¿—
    stats = analyzer.analyze_logs(args.start_date, args.end_date)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = analyzer.generate_report(args.output)
    print("\n" + report)
    
    # å¯¼å‡ºJSONç»Ÿè®¡
    if args.export_json:
        analyzer.export_stats_json(args.export_json)


if __name__ == "__main__":
    main()